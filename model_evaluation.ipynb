{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34685537",
   "metadata": {},
   "source": [
    "# Model Evaluation Exercises\n",
    "\n",
    "#### Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "\n",
    "- **In the context of this problem, what is a false positive?**\n",
    "- **In the context of this problem, what is a false negative?**\n",
    "- **How would you describe this model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc917489",
   "metadata": {},
   "source": [
    "- Positive: dog\n",
    "- Negative: cat\n",
    "\n",
    "\n",
    "- TP: Predict dog when it is actually a dog\n",
    "- TN: Predict cat when it is actually a cat\n",
    "\n",
    "\n",
    "- False Positive: Predict dog when it is actually a cat\n",
    "- False Negative: Predict cat when it is actually a dog\n",
    "\n",
    "This model is a dog classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e610a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.00%\n",
      "Recall: 86.79%\n",
      "Precision: 77.97%\n",
      "F1 Score: 82.14%\n"
     ]
    }
   ],
   "source": [
    "TP, TN, FP, FN = 46, 34, 13, 7\n",
    "accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "recall = TP/(TP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "F1_score = (2*precision*recall)/(precision+recall) \n",
    "print(f'Accuracy: {accuracy:.2%}\\nRecall: {recall:.2%}\\nPrecision: {precision:.2%}\\nF1 Score: {F1_score:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e082a14b",
   "metadata": {},
   "source": [
    "#### 3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "#### Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found in `c3.csv`.\n",
    "\n",
    "#### Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "- **An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d91115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "c3 = pd.read_csv('c3.csv')\n",
    "c3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa53cb",
   "metadata": {},
   "source": [
    "Positive: Duck is defective\n",
    "\n",
    "Negative: Duck is not defective\n",
    "\n",
    "- TP: Defective duck is predicted as defective\n",
    "- TN: Normal duck is predicted as not defective\n",
    "- FP: Normal duck is predicted as defective\n",
    "- FN: Defective duck is predicted as not defective\n",
    "\n",
    "Goal: Identify as many defective ducks as possible.\n",
    "\n",
    "We don't want to have any defective ducks identified as not defective, so the cost of false negatives is greater than the cost of false positives, and we should choose recall as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69899da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    184\n",
       "Defect        16\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0438248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3   baseline\n",
       "0  No Defect  No Defect  Defect  No Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect  No Defect\n",
       "2  No Defect  No Defect  Defect  No Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect  No Defect\n",
       "4  No Defect  No Defect  Defect  No Defect  No Defect"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3['baseline'] = 'No Defect'\n",
    "c3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cbdda76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Recall: 0.00%\n",
      "Model 1 Recall: 50.00%\n",
      "Model 2 Recall: 56.25%\n",
      "Model 3 Recall: 81.25%\n"
     ]
    }
   ],
   "source": [
    "recall_subset = c3[c3.actual == 'Defect']\n",
    "baseline_recall = (recall_subset.actual == recall_subset.baseline).mean()\n",
    "m1_recall = (recall_subset.actual == recall_subset.model1).mean()\n",
    "m2_recall = (recall_subset.actual == recall_subset.model2).mean()\n",
    "m3_recall = (recall_subset.actual == recall_subset.model3).mean()\n",
    "print(f'Baseline Recall: {baseline_recall:.2%}')\n",
    "print(f'Model 1 Recall: {m1_recall:.2%}')\n",
    "print(f'Model 2 Recall: {m2_recall:.2%}')\n",
    "print(f'Model 3 Recall: {m3_recall:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cb6a7",
   "metadata": {},
   "source": [
    "When using recall as our evaluation metric, to minimize false negatives, Model 3 is the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e118ec2",
   "metadata": {},
   "source": [
    "- **Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you they really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fa668",
   "metadata": {},
   "source": [
    "Precision would be appropriate as our evaluation metric in this case because the cost of false positives (giving a vacation to someone with a normal duck) is greater than the cost of false negatives (predicting a defective duck as not defective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19f465ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1', 'model2', 'model3']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3_cols = list(c3.columns)\n",
    "c3_cols.remove('actual')\n",
    "c3_cols.remove('baseline')\n",
    "c3_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a415ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision: 80.00%\n",
      "model2 precision: 10.00%\n",
      "model3 precision: 13.13%\n"
     ]
    }
   ],
   "source": [
    "for col in c3_cols:\n",
    "    subset = c3[c3[col] == 'Defect']\n",
    "    precision = (subset.actual == subset[col]).mean()\n",
    "    print(f'{col} precision: {precision:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66f236",
   "metadata": {},
   "source": [
    "When using precision as our evaluation metric, Model 1 is the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9d11d",
   "metadata": {},
   "source": [
    "#### 4. You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "#### At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "#### Several models have already been developed with the data, and you can find their results in `gives_you_paws.csv`.\n",
    "\n",
    "#### Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n",
    "\n",
    "#### a. In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ca6a9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv into dataframe\n",
    "paws = pd.read_csv('gives_you_paws.csv')\n",
    "paws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dc88de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    3254\n",
       "cat    1746\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most common class\n",
    "paws.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7544d6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      dog\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "4    cat    cat    cat    dog    dog      dog"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create baseline model\n",
    "paws['baseline'] = 'dog'\n",
    "paws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9e16237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1', 'model2', 'model3', 'model4', 'baseline']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of columns\n",
    "paws_cols = list(paws.columns)\n",
    "# remove 'actual' column so only models remain\n",
    "paws_cols.remove('actual')\n",
    "paws_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b173588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy: 80.74%\n",
      "model2 accuracy: 63.04%\n",
      "model3 accuracy: 50.96%\n",
      "model4 accuracy: 74.26%\n",
      "baseline accuracy: 65.08%\n"
     ]
    }
   ],
   "source": [
    "# use for loop to print accuracy of each model\n",
    "for col in paws_cols:\n",
    "    accuracy = (paws.actual == paws[col]).mean()\n",
    "    print(f'{col} accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed876fd",
   "metadata": {},
   "source": [
    "Model 3 is the least accurate model.\n",
    "Both models 2 and 3 have lower accuracy than the baseline model.\n",
    "Models 1 and 4 are more accurate than the baseline model, which only predicts 'dog'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374857e",
   "metadata": {},
   "source": [
    "#### b. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83dbc9",
   "metadata": {},
   "source": [
    "- Positive: Identified as dog\n",
    "- Negative: Identified as cat\n",
    "\n",
    "Outcomes:\n",
    "- TP: Dog identified as dog\n",
    "- TN: Cat identified as cat\n",
    "- FP: Cat identified as dog\n",
    "- FN: Dog identified as cat\n",
    "\n",
    "Goal: Correctly identify pictures of dogs.\n",
    "\n",
    "Costs:\n",
    "- FP: Cat picture is included with dog pictures\n",
    "- FN: Dog picture is not included with dog pictures.\n",
    "\n",
    "We don't want to get rid of any dog pictures since that's the group we're focusing on. We should try to minimize false negatives in Phase I and use recall as our evaluation metric.\n",
    "\n",
    "Customers are supposed to pay an additional fee to get pictures of both dogs and cats; we don't want to let that happen for free so we should try to reduce the amount of cat pictures identified as dogs (false positives) and use precision as our evaluation metric in Phase II.\n",
    "\n",
    "\n",
    "- Choose recall for Phase I.\n",
    "- Choose precision as our metric for Phase II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58a3de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 recall: 80.33%\n",
      "model2 recall: 49.08%\n",
      "model3 recall: 50.86%\n",
      "model4 recall: 95.57%\n",
      "baseline recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "for col in paws_cols:\n",
    "    subset = paws[paws.actual == 'dog']\n",
    "    recall = (subset.actual == subset[col]).mean()\n",
    "    print(f'{col} recall: {recall:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ddf2cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision: 89.00%\n",
      "model2 precision: 89.32%\n",
      "model3 precision: 65.99%\n",
      "model4 precision: 73.12%\n",
      "baseline precision: 65.08%\n"
     ]
    }
   ],
   "source": [
    "for col in paws_cols:\n",
    "    subset = paws[paws[col] == 'dog']\n",
    "    precision = (subset.actual == subset[col]).mean()\n",
    "    print(f'{col} precision: {precision:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69e7ab",
   "metadata": {},
   "source": [
    "For Phase I I would recommend Model 4 because it has the greatest recall.\n",
    "\n",
    "For Phase II I would recommend Model 2 because it has the greatest precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fb122",
   "metadata": {},
   "source": [
    "#### c. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950b6b5",
   "metadata": {},
   "source": [
    "- Positive: cat\n",
    "- Negative: dog\n",
    "\n",
    "\n",
    "- TP: Cat identified as cat\n",
    "- TN: Dog identified as dog\n",
    "- FP: Dog identified as cat\n",
    "- FN: Cat identified as dog\n",
    "\n",
    "\n",
    "- In Phase I, we do not want to miss out on any cat pictures (minimize false negatives).\n",
    "- In Phase II, we do not want to include dog pictures with cat pictures (minimize false positives).\n",
    "\n",
    "\n",
    "- We should use recall as our evaluation metric in Phase I.\n",
    "- We should use precision as our evaluation metric in Phase II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc2b3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 recall: 81.50%\n",
      "model2 recall: 89.06%\n",
      "model3 recall: 51.15%\n",
      "model4 recall: 34.54%\n",
      "baseline recall: 0.00%\n"
     ]
    }
   ],
   "source": [
    "for col in paws_cols:\n",
    "    subset = paws[paws.actual == 'cat']\n",
    "    recall = (subset.actual == subset[col]).mean()\n",
    "    print(f'{col} recall: {recall:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a89a0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision: 68.98%\n",
      "model2 precision: 48.41%\n",
      "model3 precision: 35.83%\n",
      "model4 precision: 80.72%\n",
      "baseline precision: nan%\n"
     ]
    }
   ],
   "source": [
    "for col in paws_cols:\n",
    "    subset = paws[paws[col] == 'cat']\n",
    "    precision = (subset.actual == subset[col]).mean()\n",
    "    print(f'{col} precision: {precision:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87351d73",
   "metadata": {},
   "source": [
    "For Phase I I would recommend Model 2 because it has the greatest recall.\n",
    "\n",
    "For Phase II I would recommend Model 4 because it has the greatest precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e8fd0",
   "metadata": {},
   "source": [
    "#### 5. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "- sklearn.metrics.accuracy_score\n",
    "- sklearn.metrics.precision_score\n",
    "- sklearn.metrics.recall_score\n",
    "- sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8d57f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy: 80.74%\n",
      "model2 accuracy: 63.04%\n",
      "model3 accuracy: 50.96%\n",
      "model4 accuracy: 74.26%\n",
      "baseline accuracy: 65.08%\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "# use sklearn.metrics.accuracy_score to print accuracy of each model\n",
    "for col in paws_cols:\n",
    "    print(f'{col} accuracy: {sklearn.metrics.accuracy_score(paws.actual, paws[col]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bddd67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision: 89.00%\n",
      "model2 precision: 89.32%\n",
      "model3 precision: 65.99%\n",
      "model4 precision: 73.12%\n",
      "baseline precision: 65.08%\n"
     ]
    }
   ],
   "source": [
    "# use sklearn.metrics.precision_score to print precision of each model\n",
    "for col in paws_cols:\n",
    "    precision = sklearn.metrics.precision_score(paws.actual, paws[col], pos_label='dog')\n",
    "    print(f'{col} precision: {precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c098d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 recall: 80.33%\n",
      "model2 recall: 49.08%\n",
      "model3 recall: 50.86%\n",
      "model4 recall: 95.57%\n",
      "baseline recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# use sklearn.metrics.recall_score to print recall of each model\n",
    "for col in paws_cols:\n",
    "    recall = sklearn.metrics.recall_score(paws.actual, paws[col], pos_label='dog')\n",
    "    print(f'{col} recall: {recall:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b121baa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n",
      "model2 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "         dog       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n",
      "model3 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "         dog       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n",
      "model4 classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "         dog       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n",
      "baseline classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.00      0.00      0.00      1746\n",
      "         dog       0.65      1.00      0.79      3254\n",
      "\n",
      "    accuracy                           0.65      5000\n",
      "   macro avg       0.33      0.50      0.39      5000\n",
      "weighted avg       0.42      0.65      0.51      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# use sklearn.metrics.classification_report to print classification report for each model\n",
    "for col in paws_cols:\n",
    "    print(f'{col} classification report:\\n{sklearn.metrics.classification_report(paws.actual, paws[col])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e655e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
