{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**What is a Random Forest?**\n",
    "\n",
    "- Random Forest is a type **Ensemble** Machine Learning algorithm called Bootstrap Aggregation or bagging.\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "- **Bootstrapping** is a statistical method for estimating a quantity from a data sample, e.g. mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value. In bagging, the same approach is used for estimating entire statistical models, such as decision trees. Multiple samples of your training data are taken and models are constructed for each sample set. When you need to make a prediction for new data, each model makes a prediction and the **predictions are averaged** to give a better estimate of the true output value.\n",
    "\n",
    "- Random forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.  The models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.\n",
    "\n",
    "- In the example below, \"Will I exercise\", \"I\" am a single observation. Each person is an observation. The model takes each observation through the forest and votes on the most frequent class for that observation to get a final prediction. \n",
    "\n",
    "\n",
    "**Pros**\n",
    "\n",
    "1. Reduction in over-fitting \n",
    "\n",
    "2. More accurate than decision trees in most cases\n",
    "\n",
    "3. Naturally performs feature selection\n",
    "\n",
    "**Cons**\n",
    "\n",
    "1. Slow real time prediction\n",
    "\n",
    "2. Difficult to implement\n",
    "\n",
    "3. Complex algorithm so difficult to explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slide Show: https://docs.google.com/presentation/d/1tXxpl8XUzg-nBnlgoE2y-0qR2BMcZJ4o0rCRWxUECUw/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "1           5.1          3.5           1.4          0.2  setosa\n",
       "2           4.9          3.0           1.4          0.2  setosa\n",
       "3           4.7          3.2           1.3          0.2  setosa\n",
       "4           4.6          3.1           1.5          0.2  setosa\n",
       "5           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read Iris data from pydatset\n",
    "df = data('iris')\n",
    "\n",
    "# convert column names to lowercase, replace '.' in column names with '_'\n",
    "df.columns = [col.lower().replace('.', '_') for col in df]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validate Test\n",
    "\n",
    "Now we'll do our train/validate/test split:\n",
    "\n",
    "- We'll do exploration and train our model on the `train` data\n",
    "\n",
    "- We tune our model on `validate`, since it will be out-of-sample until we use it.\n",
    "\n",
    "- And keep the `test` nice and safe and separate, for our final out-of-sample dataset, to see how well our tuned model performs on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_validate_test_split(df, target, seed=123):\n",
    "    '''\n",
    "    This function takes in a dataframe, the name of the target variable\n",
    "    (for stratification purposes), and an integer for a setting a seed\n",
    "    and splits the data into train, validate and test. \n",
    "    Test is 20% of the original dataset, validate is .30*.80= 24% of the \n",
    "    original dataset, and train is .70*.80= 56% of the original dataset. \n",
    "    The function returns, in this order, train, validate and test dataframes. \n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=0.2, \n",
    "                                            random_state=seed, \n",
    "                                            stratify=df[target])\n",
    "    train, validate = train_test_split(train_validate, test_size=0.3, \n",
    "                                       random_state=seed,\n",
    "                                       stratify=train_validate[target])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, validate, test\n",
    "train, validate, test = train_validate_test_split(df, target='species', seed=123)\n",
    "\n",
    "# create X & y version of train, where y is a series with just the target variable and X are all the features. \n",
    "\n",
    "X_train = train.drop(columns=['species'])\n",
    "y_train = train.species\n",
    "\n",
    "X_validate = validate.drop(columns=['species'])\n",
    "y_validate = validate.species\n",
    "\n",
    "X_test = test.drop(columns=['species'])\n",
    "y_test = test.species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "**Create the object**\n",
    "\n",
    "Create the Random Forest object with desired hyper-parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=3, \n",
    "                            random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=3, random_state=123)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**\n",
    "\n",
    "Fit the random forest algorithm to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=3, random_state=123)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**\n",
    "\n",
    "Evaluate importance, or weight, of each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08414923 0.03125852 0.48344252 0.40114973]\n"
     ]
    }
   ],
   "source": [
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The higher number the feature importance == more important the feature.\n",
    "- The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions**\n",
    "\n",
    "Classify each flower by its estimated species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['versicolor', 'setosa', 'virginica', 'versicolor', 'setosa',\n",
       "       'virginica', 'setosa', 'setosa', 'setosa', 'virginica',\n",
       "       'versicolor', 'virginica', 'virginica', 'versicolor', 'virginica',\n",
       "       'virginica', 'setosa', 'versicolor', 'setosa', 'virginica',\n",
       "       'versicolor', 'virginica', 'versicolor', 'versicolor', 'setosa',\n",
       "       'versicolor', 'versicolor', 'setosa', 'versicolor', 'setosa',\n",
       "       'versicolor', 'setosa', 'virginica', 'setosa', 'setosa',\n",
       "       'versicolor', 'setosa', 'virginica', 'setosa', 'setosa', 'setosa',\n",
       "       'versicolor', 'setosa', 'virginica', 'versicolor', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'setosa',\n",
       "       'versicolor', 'setosa', 'setosa', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'setosa', 'setosa', 'setosa',\n",
       "       'virginica', 'versicolor', 'virginica', 'virginica', 'versicolor',\n",
       "       'versicolor', 'virginica', 'versicolor', 'setosa', 'virginica',\n",
       "       'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica',\n",
       "       'setosa', 'versicolor', 'virginica', 'setosa', 'versicolor',\n",
       "       'versicolor', 'setosa', 'versicolor'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf.predict(X_train)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate Probability**\n",
    "\n",
    "Estimate the probability of each species, using the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [9.60967742e-01, 3.63936026e-02, 2.63865546e-03],\n",
       "       [0.00000000e+00, 3.35430165e-01, 6.64569835e-01],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [9.90967742e-01, 9.03225806e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 7.22624432e-03, 9.92773756e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 3.35456888e-02, 9.66454311e-01],\n",
       "       [9.67741935e-04, 9.80839585e-01, 1.81926733e-02],\n",
       "       [0.00000000e+00, 5.55957765e-03, 9.94440422e-01],\n",
       "       [0.00000000e+00, 1.63929110e-02, 9.83607089e-01],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [0.00000000e+00, 3.38576174e-01, 6.61423826e-01],\n",
       "       [0.00000000e+00, 1.56567020e-01, 8.43432980e-01],\n",
       "       [9.90967742e-01, 9.03225806e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 5.99335379e-01, 4.00664621e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e-02, 1.97262443e-02, 9.70273756e-01],\n",
       "       [9.67741935e-04, 9.77295701e-01, 2.17365575e-02],\n",
       "       [0.00000000e+00, 4.99291104e-01, 5.00708896e-01],\n",
       "       [9.67741935e-04, 9.47173081e-01, 5.18591766e-02],\n",
       "       [2.00000000e-02, 9.71668204e-01, 8.33179563e-03],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.67741935e-04, 9.68029549e-01, 3.10027092e-02],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.67741935e-04, 9.51771776e-01, 4.72604823e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.27123554e-02, 9.87287645e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.88316890e-01, 8.11683110e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.64623554e-02, 9.83537645e-01],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [0.00000000e+00, 2.35878952e-01, 7.64121048e-01],\n",
       "       [0.00000000e+00, 1.63929110e-02, 9.83607089e-01],\n",
       "       [0.00000000e+00, 9.93457765e-03, 9.90065422e-01],\n",
       "       [0.00000000e+00, 1.21682100e-01, 8.78317900e-01],\n",
       "       [0.00000000e+00, 3.64921377e-01, 6.35078623e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.00000000e-02, 9.46231696e-01, 3.37683036e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 5.55957765e-03, 9.94440422e-01],\n",
       "       [0.00000000e+00, 3.09444983e-02, 9.69055502e-01],\n",
       "       [0.00000000e+00, 5.55957765e-03, 9.94440422e-01],\n",
       "       [0.00000000e+00, 1.72262443e-02, 9.82773756e-01],\n",
       "       [9.90967742e-01, 9.03225806e-03, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e-02, 1.97262443e-02, 9.70273756e-01],\n",
       "       [9.67741935e-04, 9.70804034e-01, 2.82282242e-02],\n",
       "       [0.00000000e+00, 3.09444983e-02, 9.69055502e-01],\n",
       "       [0.00000000e+00, 3.58092188e-01, 6.41907812e-01],\n",
       "       [9.67741935e-04, 9.61625299e-01, 3.74069590e-02],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [0.00000000e+00, 3.80972761e-02, 9.61902724e-01],\n",
       "       [9.67741935e-04, 9.68026256e-01, 3.10060020e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.21869745e-01, 8.78130255e-01],\n",
       "       [9.67741935e-04, 9.38561970e-01, 6.04702877e-02],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [1.00000000e-02, 3.06290221e-02, 9.59370978e-01],\n",
       "       [0.00000000e+00, 1.43790221e-02, 9.85620978e-01],\n",
       "       [0.00000000e+00, 1.27123554e-02, 9.87287645e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [0.00000000e+00, 7.22624432e-03, 9.92773756e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.09677419e-02, 9.24319797e-01, 6.47124614e-02],\n",
       "       [9.67741935e-04, 9.87061807e-01, 1.19704511e-02],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.67741935e-04, 9.80839585e-01, 1.81926733e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = rf.predict_proba(X_train)\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "**Compute the Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier on training set: 0.98\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  0  0]\n",
      " [ 0 26  2]\n",
      " [ 0  0 28]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a classificaiton report**\n",
    "\n",
    "\n",
    "**Precision:** $\\frac{TP}{(TP + FP)}$\n",
    "\n",
    "**Recall:** $\\frac{TP}{(TP + FN)}$\n",
    "\n",
    "**F1-Score:** A measure of accuracy. The harmonic mean of precision & recall. The harmonic mean is the reciprocal of the arithmetic mean of the reciprocals.  \n",
    "\n",
    "F1 $\\in [0, 1]$\n",
    "\n",
    "F1-score = harmonic mean = $\\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$\n",
    "\n",
    "**Support:** number of occurrences of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        28\n",
      "  versicolor       1.00      0.93      0.96        28\n",
      "   virginica       0.93      1.00      0.97        28\n",
      "\n",
      "    accuracy                           0.98        84\n",
      "   macro avg       0.98      0.98      0.98        84\n",
      "weighted avg       0.98      0.98      0.98        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model\n",
    "\n",
    "**Evaluate on Out-of-Sample data**\n",
    "\n",
    "Compute the accuracy of the model when run on the validate dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier on test set: 0.97\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of random forest classifier on test set: {:.2f}'\n",
    "     .format(rf.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Continue working in your `model` file with titanic data to do the following: \n",
    "\n",
    "1. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10.\n",
    "\n",
    "1. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "\n",
    "1. Print and clearly label the following:  Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    "\n",
    "1. Run through steps increasing your min_samples_leaf and decreasing your max_depth. \n",
    "\n",
    "1. What are the differences in the evaluation metrics?  Which performs better on your in-sample data?  Why?\n",
    "\n",
    "After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
